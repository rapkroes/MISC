---
title: "Home Assignment Noth"
author: "Raphael Kroes"
date: "2024-12-04"
output: html_document
---

The 'jsonlite' library is required only if the API call is executed. I advise against it, as it has very long run time.
```{r}
## the 'conflicted' library from GitHub is recommended, but not mandatory
library(devtools)
library(conflicted)
API_call<- FALSE
if(isTRUE(API_call)){
  library(jsonlite)
}
library(tidyverse)
library(lme4)
library(bacondecomp) # loads, but data does not work with the package
# library(DIDmultiplegt) # does not work as I do not have root access to update my Java
# devtools::install_github("jonathandroth/pretrends")
library(pretrends)
library(HonestDiD)
```
The following code contains the API call. Run it at your own peril - it will take some time. 
```{r}
if(isTRUE(API_call)){
  #curate bank data
  data_list<- list()
  pb<- txtProgressBar(min = 1, max = 200, )
  for(i in seq(1, 165)){
    offset_i<- format((i - 1) * 10000, scientific = FALSE)
    url_i<- url(description  = paste0("http://banks.data.fdic.gov/api/financials?fields=CERT,STALP,STCNTY,STNAME,ROA,REPDTE&limit=1000&offset=", offset_i))
    im<- fromJSON(url_i)
    data_list[[i]]<- im$data$data
    setTxtProgressBar(pb, i)
  }
  close(pb)
}
```
I conducted minor data preparation on the county adjacency data using MS Notepad. I separated the county and states into individual columns and renamed the columns. For convenience, I uploaded the adjacency data and the branching index data to a GitHub file. 
```{r}
adjacency_data<- read.csv("https://raw.githubusercontent.com/rapkroes/MISC/refs/heads/main/county_adjacency2024.csv", sep = ";")
branching_index_data<- read.csv("https://github.com/rapkroes/MISC/raw/refs/heads/main/johnson_rice_branching_index.csv")
if(isTRUE(API_call)){
  bank_data<- bind_rows(data_list)
}else{
  bank_data<- read.csv("https://github.com/rapkroes/MISC/raw/refs/heads/main/raw_bank_data.csv")
}
```


```{r}
# 1
bank_data<- bank_data|> 
  #a
  mutate(REPDTE = as.numeric(REPDTE))|>
  mutate(YEAR = as.numeric(substr(REPDTE, 1, 4)))|>
  mutate(MONTH = as.numeric(substr(REPDTE, 5, 6)))|>
  mutate(DAY = as.numeric(substr(REPDTE, 7, 8)))|>
  mutate(QUARTER = ceiling(MONTH / 3))|>
  #b
  dplyr::filter(STCNTY != "null" & STNAME != "" & STALP != "")|>
  #c
  mutate(STNAME = if_else(STNAME == "DISTRICT OF COLUMBIA", "DC", STNAME))|>
  #d 
  mutate(location = paste(STALP, str_to_lower(STCNTY), sep = "_"))|>
  mutate(location = str_remove_all(location, " "))|>
  mutate(location = str_remove_all(location, "\\."))|>
  mutate(location = if_else(location == "MO_saintlouis(city)", 
                            "MO_stlouis",
                            location))|>
  arrange(location)
```
The following code leads to perfect matching between the bank data and the adjacency data.
```{r}
# 1e
adj_addage<- adjacency_data|>
  mutate(bordercounty = as.numeric(state_1 != state_2))|>
  mutate(county = str_to_lower(county_1))|>
  mutate(county = sub(" city and borough", "", county))|>
  mutate(county = sub(" county", "", county))|>
  mutate(county = sub(" parish", "", county))|>
  mutate(county = sub(" borough", "", county))|>
  mutate(county = sub(" census area", "", county))|>
  mutate(county = sub(" municipality", "", county))|>
  mutate(county = sub(" district", "", county))|>
  mutate(county = sub(" island", "", county))|>
  mutate(state = str_squish(state_1))|>
  mutate(location = paste(state, county, sep = "_"))|>
  select(county, bordercounty, location)|>
  distinct()|>
  # the following counties do not appear in the adjacency data and were manually added using Google Maps.
  add_row(county = "fairfield", bordercounty = 0, location = "CT_fairfield")|>
  add_row(county = "hartford", bordercounty = 0, location = "CT_hartford")|>
  add_row(county = "litchfield", bordercounty = 0, location = "CT_litchfield")|>
  add_row(county = "middlesex", bordercounty = 0, location = "CT_middlesex")|>
  add_row(county = "new haven", bordercounty = 0, location = "CT_new haven")|>
  add_row(county = "new london", bordercounty = 0, location = "CT_new london")|>
  add_row(county = "tolland", bordercounty = 0, location = "CT_tolland")|>
  add_row(county = "windham", bordercounty = 0, location = "CT_windham")|>
  add_row(county = "kosrae", bordercounty = 0, location = "FM_kosrae")|>
  add_row(county = "pohnpei", bordercounty = 0, location = "FM_pohnpei")|>
  add_row(county = "arecibo", bordercounty = 0, location = "PR_arecibo")|>
  add_row(county = "bayamon", bordercounty = 0, location = "PR_bayamon")|>
  add_row(county = "caguas", bordercounty = 0, location = "PR_caguas")|>
  add_row(county = "carolina", bordercounty = 0, location = "PR_carolina")|>
  add_row(county = "guaynabo", bordercounty = 0, location = "PR_guaynabo")|>
  add_row(county = "humacao", bordercounty = 0, location = "PR_humacao")|>
  add_row(county = "mayaguez", bordercounty = 0, location = "PR_mayaguez")|>
  add_row(county = "ponce", bordercounty = 0, location = "PR_ponce")|>
  add_row(county = "sanjuan", bordercounty = 0, location = "PR_sanjuan")|>
  # removal of special characters
  mutate(location = str_remove_all(location, " "))|>
  mutate(location = str_remove_all(location, "\\."))|>
  mutate(location = if_else(location == "NM_doÃ±aana", "NM_donaana", location))|>
  # mediate other differences in notation
  mutate(
    location = if_else(location == "VA_bristolcity", "VA_bristol", location)
  )|>
  mutate(location = if_else(location == "VA_salemcity", "VA_salem", location))|>
  mutate(location = if_else(location == "IL_rock", "IL_rockisland", location))|>
  dplyr::filter(bordercounty == 1)|>
  arrange(location)

joint_data_1<- left_join(bank_data, adj_addage, by = "location")|>
  dplyr::filter(bordercounty == 1)|>
  select(- bordercounty, - county)

```
The problem set does not specify what to do about policy changes that went back (i.e. tightening of regulation) and changes that did not change the index. We therefore choose to ignore the policy changes that left the index unaffected and ignore tightening regulation when it comes to the 'first deregulation' index.
```{r}
# 1f and 2 (jointly)
state_vector<- unique(joint_data_1$STNAME)

branching_index_up<- branching_index_data|>
  mutate(YEAR = as.numeric(substr(Effective.Date, 1, 4)))|>
  mutate(branching_index_new = 4 - Branching.Restrictivness.Index)|>
  mutate(STNAME = str_to_upper(State))|>
  dplyr::filter(STNAME %in% state_vector)|>
  select(STNAME, YEAR, branching_index_new)|>
  group_by(STNAME)|>
  arrange(YEAR)|>
  mutate(first_dereg = 0)|>
  mutate(first_dereg = 
           if_else(branching_index_new != 0 & 
                     row_number() == which(branching_index_new != 0)[1], 
                   1, 
                   first_dereg)
         )|>
  dplyr::filter(c(TRUE, branching_index_new[-1] != branching_index_new[-n()]))|>
  dplyr::filter(
    branching_index_new != 0 | first_dereg != 0 | row_number() != 1
    )|>
  ungroup()|>
  arrange(STNAME)

year_vector<- seq(1990, 2006)
state_year_list<- list()
current_status<- tibble(
  STNAME = state_vector,
  branching_index_new = numeric(length(state_vector)),
  YEAR = numeric(length(state_vector)),
  deregulated = numeric(length(state_vector)),
  first_dereg = numeric(length(state_vector))
)
for(i in seq_along(year_vector)){
  im<- branching_index_up|>
    dplyr::filter(YEAR == year_vector[i])
  current_status<- current_status|>
    mutate(first_dereg = 0)
  if(nrow(im) > 0){
    index<- which(state_vector %in% im$STNAME)
    current_status<- current_status|>
      mutate(branching_index_new = 
               replace(branching_index_new, index, im$branching_index_new)
             )|>
      mutate(deregulated = replace(deregulated, index, 1))|>
      mutate(first_dereg = replace(first_dereg, index, im$first_dereg))
  }
  current_status<- current_status|>
    mutate(YEAR = year_vector[i])
  state_year_list[[i]]<- current_status
}
state_year_tibble<- state_year_list|>
  bind_rows()|>
  arrange(STNAME)

joint_data_2<- left_join(joint_data_1, state_year_tibble, by = c("STNAME", "YEAR"))|>
  # g
  dplyr::filter(YEAR >= 1990 & YEAR <= 2006)|>
  # h
  group_by(CERT)|>
  arrange(REPDTE)|>
  dplyr::filter(n() == 17 * 4)|>
  dplyr::filter(QUARTER == rep(c(1, 2, 3, 4), 17))|>
  ungroup()|>
  # i
  group_by(CERT)|>
  dplyr::filter(all(STALP == STALP[1]))|>
  ungroup()

print(list(nobs_year = table(joint_data_2$YEAR), 
           no_banks = length(unique(joint_data_2$CERT)), 
           no_counties = length(unique(joint_data_2$STCNTY))
           ))


```
```{r}
dereg_waves<- joint_data_2|>
  select(CERT, YEAR, first_dereg)|>
  dplyr::filter(first_dereg == 1)|>
  distinct()|>
  group_by(YEAR)|>
  summarise(no_banks = n())

dereg_plot<- ggplot(dereg_waves, aes(x = YEAR, y = no_banks)) +
  geom_bar(stat = "identity") +
  scale_x_continuous(breaks = seq(1990, 2006, by = 2), limits = c(1990, 2006)) +
  theme_light() +
  labs(title = "Number of Banks That Become Deregulated, 1990-2006") +
  xlab("year") +
  ylab("number of affected banks")
dereg_plot

dereg_by_year<- joint_data_2|>
  select(CERT, YEAR, deregulated)|>
  group_by(YEAR)|>
  summarise(proportion = mean(deregulated))

dereged_prop_plot<- ggplot(dereg_by_year, aes(x = YEAR, y = proportion)) +
  geom_line() +
  scale_x_continuous(breaks = seq(1990, 2006, by = 2), limits = c(1990, 2006)) +
  theme_light() +
  labs(title = "Proportion of Banks That Are Deregulated, 1990-2006",
       caption = "Note that we only consider banks in states that (i) deregulated 
       between 1990 and 2006 (ii) with headquarters in counties bordering on a 
       neighbour state.") +
  xlab("year") +
  ylab("proportion of affected banks")
dereged_prop_plot
```
There is basically only one staggered (bi-modal) wave between 1995 and 1997 for first deregulations. After this, all of the considered banks are under some kind of deregulated regime.
```{r}
lm_data<- joint_data_2|>
  mutate(CERT = as.factor(CERT))|>
  mutate(YEAR = as.factor(YEAR))|>
  mutate(STNAME = as.factor(STNAME))|>
  mutate(QUARTER = as.factor(QUARTER))

DID_1<- lmer(ROA ~ first_dereg + (1 | YEAR)  + (1 | STNAME) + (1 | CERT), data = lm_data)

summary(DID_1)
```
3
Note that I include STNAME in the regression. STNAME is to account for local state effects. I admit to not having a clue if this model is overly reasonable on a general level - I am not familiar with the data generation process (DGP) of the data used for this study. Understanding the DGP is essential for defensible modelling. Anyway, I do not consider leaving out the variable by which I cluster a defensible choice, even though it is theoretically possible.
The DID coefficient is given as approximately 0.005 with a standard deviation of 0.03. That is to say, assuming the model is approximately correctly specified, we expect the deregulation of the branching of banks to increase return over assets on average by 0.5%; if the model is approximately correctly specified, we expect the true effect to lie with a probability of about 95% between -0.06 and 0.07. Considering the 25% quantile of ROA is 0.72 and the 75% quantile is 1.38, the effect of deregulation has in this case only very small effects on the ROA.

4
The diff-in-diff estimator in (3) has a problem accounting for the different timing of the treatment. If every US state were modelled entirely independent from each other, this would not be a problem. However, the model ties the states together using random (YEAR) effects. The staggered treatment coincides with three different random effects, as there are three points in time when the treatment (according to this crude model) is administered. According to the lecture slides, this can be formulated as a multiple comparison problem violating the parallel trends assumption.
```{r}
# 5

lm_data_2<- joint_data_2|>
  select(ROA, first_dereg, STNAME, CERT, YEAR, QUARTER)|>
  mutate(STNAME = fct_inorder(STNAME))|>
  pivot_wider(names_from = STNAME, values_from = STNAME, 
              values_fn = list(STNAME = ~ 1), values_fill = list(STNAME = 0))|>
  mutate(year_quarter = 10 * YEAR + QUARTER)|>
  select(- YEAR, - QUARTER)
# DID_2<- bacon(ROA ~ first_dereg + . -FE1 -FE2, data = lm_data_2, id_var = "CERT", time_var = "year_quarter")
# apparently, the bacondecomp package does not work properly, as CERT is in the data. We will do the decomposition in the following manually

```

```{r}
# 6


```
7
The parallel trends assumption is the foundation of difference in differences. In the real world, we observe a treatment and a control group. But we really want, though, is to know 'What would have happened if the treatment group had not been treated? How much of a difference does the treatment make on the outcome?' To answer this, one may employ an untreated proxy. The proxy is in some ways similar and in some ways different to the treated unit. Because of those differences, we cannot say 'Had the treated unit not been treated, it would have ended like the control unit!' However, due to the similarities between treatment and control unit, we may be willing to bite the bullet of accepting parallel trends. Parallel trends mean that even though the status of the control/ treatment group might differ, their dynamics are similar. DID assumes identical, linear dynamics. Whether this is a defensible modeling choice must be evaluated case-by-case. 
```{r}
# https://github.com/jonathandroth/pretrends/blob/master/README.md
slope80<- slope_for_power(sigma = NA, targetPower = 0.8, tVec = NA, referencePeriod = NA)
pretrends(betahat = NA, sigma = NA, deltatrue = slope80, tVec = NA)

```

