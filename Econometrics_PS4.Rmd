---
title: "Home Assignment Noth"
author: "Raphael Kroes"
date: "2024-12-04"
output: html_document
---

The 'jsonlite' library is required only if the API call is executed. I advise against it, as it has very long run time.
bacondecomp does not work with the data. The reason is simple: The data is not balanced in the sense of package. We have four observations per unit per year. The package checks whether there is one observation per unit per year and responds with an error because this criterion is not fulfilled. The only work-around I see would have been to use only individual quarters; however, this would be a major change to the data. As the assignment does not mention such a change, I do not apply it and will discuss the results without the 'true' Bacon-Goodman decomposition as it cannot be applied.
```{r}
## the 'conflicted' library from GitHub is recommended, but not mandatory
library(devtools)
library(conflicted)
API_call<- FALSE
if(isTRUE(API_call)){
  library(jsonlite)
}
library(tidyverse)
library(lme4)
# library(bacondecomp)
# library(DIDmultiplegt) # does not work as I do not have root access to update my Java
# devtools::install_github("jonathandroth/pretrends")
library(pretrends)
library(HonestDiD)
```
The following code contains the API call. Run it at your own peril - it will take some time. 
```{r}
if(isTRUE(API_call)){
  #curate bank data
  data_list<- list()
  pb<- txtProgressBar(min = 1, max = 200)
  for(i in seq(1, 165)){
    offset_i<- format((i - 1) * 10000, scientific = FALSE)
    url_i<- url(description  = paste0("http://banks.data.fdic.gov/api/financials?fields=CERT,STALP,STCNTY,STNAME,ROA,REPDTE&limit=1000&offset=", offset_i))
    im<- fromJSON(url_i)
    data_list[[i]]<- im$data$data
    setTxtProgressBar(pb, i)
  }
  close(pb)
}
```
I conducted minor data preparation on the county adjacency data using MS Notepad. I separated the county and states into individual columns and renamed the columns. For convenience, I uploaded the adjacency data and the branching index data to a GitHub file. 
```{r}
adjacency_data<- read.csv("https://raw.githubusercontent.com/rapkroes/MISC/refs/heads/main/county_adjacency2024.csv", sep = ";")
branching_index_data<- read.csv("https://github.com/rapkroes/MISC/raw/refs/heads/main/johnson_rice_branching_index.csv")
if(isTRUE(API_call)){
  bank_data<- bind_rows(data_list)
}else{
  bank_data_1<- read.csv("https://github.com/rapkroes/MISC/raw/refs/heads/main/raw_bank_data_1.csv")
  bank_data_2<- read.csv("https://github.com/rapkroes/MISC/raw/refs/heads/main/raw_bank_data_2.csv")
  bank_data<- bind_rows(list(bank_data_1, bank_data_2))
}
```


```{r}
# 1
bank_data<- bank_data|> 
  #a
  mutate(REPDTE = as.numeric(REPDTE))|>
  mutate(YEAR = as.numeric(substr(REPDTE, 1, 4)))|>
  mutate(MONTH = as.numeric(substr(REPDTE, 5, 6)))|>
  mutate(DAY = as.numeric(substr(REPDTE, 7, 8)))|>
  mutate(QUARTER = ceiling(MONTH / 3))|>
  #b
  dplyr::filter(STCNTY != "null" & STNAME != "" & STALP != "")|>
  #c
  mutate(STNAME = if_else(STNAME == "DISTRICT OF COLUMBIA", "DC", STNAME))|>
  #d 
  mutate(location = paste(STALP, str_to_lower(STCNTY), sep = "_"))|>
  mutate(location = str_remove_all(location, " "))|>
  mutate(location = str_remove_all(location, "\\."))|>
  mutate(location = if_else(location == "MO_saintlouis(city)", 
                            "MO_stlouis",
                            location))|>
  arrange(location)
```
The following code leads to perfect matching between the bank data and the adjacency data.
```{r}
# 1e
adj_addage<- adjacency_data|>
  mutate(bordercounty = as.numeric(state_1 != state_2))|>
  mutate(county = str_to_lower(county_1))|>
  mutate(county = sub(" city and borough", "", county))|>
  mutate(county = sub(" county", "", county))|>
  mutate(county = sub(" parish", "", county))|>
  mutate(county = sub(" borough", "", county))|>
  mutate(county = sub(" census area", "", county))|>
  mutate(county = sub(" municipality", "", county))|>
  mutate(county = sub(" district", "", county))|>
  mutate(county = sub(" island", "", county))|>
  mutate(state = str_squish(state_1))|>
  mutate(location = paste(state, county, sep = "_"))|>
  select(county, bordercounty, location)|>
  distinct()|>
  # the following counties do not appear in the adjacency data and were manually added using Google Maps.
  add_row(county = "fairfield", bordercounty = 0, location = "CT_fairfield")|>
  add_row(county = "hartford", bordercounty = 0, location = "CT_hartford")|>
  add_row(county = "litchfield", bordercounty = 0, location = "CT_litchfield")|>
  add_row(county = "middlesex", bordercounty = 0, location = "CT_middlesex")|>
  add_row(county = "new haven", bordercounty = 0, location = "CT_new haven")|>
  add_row(county = "new london", bordercounty = 0, location = "CT_new london")|>
  add_row(county = "tolland", bordercounty = 0, location = "CT_tolland")|>
  add_row(county = "windham", bordercounty = 0, location = "CT_windham")|>
  add_row(county = "kosrae", bordercounty = 0, location = "FM_kosrae")|>
  add_row(county = "pohnpei", bordercounty = 0, location = "FM_pohnpei")|>
  add_row(county = "arecibo", bordercounty = 0, location = "PR_arecibo")|>
  add_row(county = "bayamon", bordercounty = 0, location = "PR_bayamon")|>
  add_row(county = "caguas", bordercounty = 0, location = "PR_caguas")|>
  add_row(county = "carolina", bordercounty = 0, location = "PR_carolina")|>
  add_row(county = "guaynabo", bordercounty = 0, location = "PR_guaynabo")|>
  add_row(county = "humacao", bordercounty = 0, location = "PR_humacao")|>
  add_row(county = "mayaguez", bordercounty = 0, location = "PR_mayaguez")|>
  add_row(county = "ponce", bordercounty = 0, location = "PR_ponce")|>
  add_row(county = "sanjuan", bordercounty = 0, location = "PR_sanjuan")|>
  # removal of special characters
  mutate(location = str_remove_all(location, " "))|>
  mutate(location = str_remove_all(location, "\\."))|>
  mutate(location = if_else(location == "NM_doÃ±aana", "NM_donaana", location))|>
  # mediate other differences in notation
  mutate(
    location = if_else(location == "VA_bristolcity", "VA_bristol", location)
  )|>
  mutate(location = if_else(location == "VA_salemcity", "VA_salem", location))|>
  mutate(location = if_else(location == "IL_rock", "IL_rockisland", location))|>
  dplyr::filter(bordercounty == 1)|>
  arrange(location)

joint_data_1<- left_join(bank_data, adj_addage, by = "location")|>
  dplyr::filter(bordercounty == 1)|>
  select(- bordercounty, - county)

```
The problem set does not specify what to do about policy changes that went back (i.e. tightening of regulation) and changes that did not change the index. We therefore choose to ignore the policy changes that left the index unaffected and ignore tightening regulation when it comes to the 'first deregulation' index.
```{r}
# 1f and 2 (jointly)
state_vector<- unique(joint_data_1$STNAME)

branching_index_up<- branching_index_data|>
  mutate(YEAR = as.numeric(substr(Effective.Date, 1, 4)))|>
  mutate(branching_index_new = 4 - Branching.Restrictivness.Index)|>
  mutate(STNAME = str_to_upper(State))|>
  dplyr::filter(STNAME %in% state_vector)|>
  select(STNAME, YEAR, branching_index_new)|>
  group_by(STNAME)|>
  arrange(YEAR)|>
  mutate(first_dereg = 0)|>
  mutate(first_dereg = 
           if_else(branching_index_new != 0 & 
                     row_number() == which(branching_index_new != 0)[1], 
                   1, 
                   first_dereg)
         )|>
  dplyr::filter(c(TRUE, branching_index_new[-1] != branching_index_new[-n()]))|>
  dplyr::filter(
    branching_index_new != 0 | first_dereg != 0 | row_number() != 1
    )|>
  ungroup()|>
  arrange(STNAME)

year_vector<- seq(1990, 2006)
state_year_list<- list()
current_status<- tibble(
  STNAME = state_vector,
  branching_index_new = numeric(length(state_vector)),
  YEAR = numeric(length(state_vector)),
  deregulated = numeric(length(state_vector)),
  first_dereg = numeric(length(state_vector))
)
for(i in seq_along(year_vector)){
  im<- branching_index_up|>
    dplyr::filter(YEAR == year_vector[i])
  current_status<- current_status|>
    mutate(first_dereg = 0)
  if(nrow(im) > 0){
    index<- which(state_vector %in% im$STNAME)
    current_status<- current_status|>
      mutate(branching_index_new = 
               replace(branching_index_new, index, im$branching_index_new)
             )|>
      mutate(deregulated = replace(deregulated, index, 1))|>
      mutate(first_dereg = replace(first_dereg, index, im$first_dereg))
  }
  current_status<- current_status|>
    mutate(YEAR = year_vector[i])
  state_year_list[[i]]<- current_status
}
state_year_tibble<- state_year_list|>
  bind_rows()|>
  arrange(STNAME)

joint_data_2<- left_join(joint_data_1, state_year_tibble, by = c("STNAME", "YEAR"))|>
  # g
  dplyr::filter(YEAR >= 1990 & YEAR <= 2006)|>
  # h
  group_by(CERT)|>
  arrange(REPDTE)|>
  dplyr::filter(n() == 17 * 4)|>
  dplyr::filter(QUARTER == rep(c(1, 2, 3, 4), 17))|>
  ungroup()|>
  # i
  group_by(CERT)|>
  dplyr::filter(all(STALP == STALP[1]))|>
  ungroup()

print(list(nobs_year = table(joint_data_2$YEAR), 
           no_banks = length(unique(joint_data_2$CERT)), 
           no_counties = length(unique(joint_data_2$STCNTY))
           ))


```
nobs_year = number of observations for every year. Note that by balancing the panel we have 4 observations per bank per year.
no_banks = number of banks in the filtered sample
no_counties = number of counties, across which the bank headquarters were spread during the sample period.
```{r}
dereg_waves<- joint_data_2|>
  select(CERT, YEAR, first_dereg)|>
  dplyr::filter(first_dereg == 1)|>
  distinct()|>
  group_by(YEAR)|>
  summarise(no_banks = n())

dereg_plot<- ggplot(dereg_waves, aes(x = YEAR, y = no_banks)) +
  geom_bar(stat = "identity") +
  scale_x_continuous(breaks = seq(1990, 2006, by = 2), limits = c(1990, 2006)) +
  theme_light() +
  labs(title = "Number of Banks That Become Deregulated, 1990-2006") +
  xlab("year") +
  ylab("number of affected banks")
dereg_plot

dereg_by_year<- joint_data_2|>
  select(CERT, YEAR, deregulated)|>
  group_by(YEAR)|>
  summarise(proportion = mean(deregulated))

dereged_prop_plot<- ggplot(dereg_by_year, aes(x = YEAR, y = proportion)) +
  geom_line() +
  scale_x_continuous(breaks = seq(1990, 2006, by = 2), limits = c(1990, 2006)) +
  theme_light() +
  labs(title = "Proportion of Banks That Are Deregulated, 1990-2006",
       caption = "Note that we only consider banks in states that (i) deregulated 
       between 1990 and 2006 (ii) with headquarters in counties bordering on a 
       neighbour state.") +
  xlab("year") +
  ylab("proportion of affected banks")
dereged_prop_plot
```
There were basically two waves of initial deregulations: One starting in 1995 and ending in 1997, and one starting in 1999 ending in 2000. The first wave affected a lot of banks, nearly 70% of the banks in the sample. The second wave had a much smaller impact, only affecting around 10% of the sampled banks.
```{r}
#additional data preparation for the different models
dereged_states<- unique(branching_index_up$STNAME)
earlyT_states<- branching_index_up|>
  dplyr::filter(YEAR <= 1997)|>
  select(STNAME)|>
  unique()|>
  unlist()

lateT_states<- branching_index_up|>
  dplyr::filter(YEAR %in% c(1999, 2000))|>
  select(STNAME)|>
  unique()|>
  unlist()

lm_data<- joint_data_2|>
  mutate(CERT = as.factor(CERT))|>
  mutate(YEARf = as.factor(YEAR))|>
  mutate(STNAME = as.factor(STNAME))|>
  mutate(QUARTERf = as.factor(QUARTER))|>
  mutate(dereg_YEAR = first_dereg * YEAR)|>
  mutate(YEAR_2 = YEAR)|>
  pivot_wider(names_from = YEAR_2, values_from = dereg_YEAR, 
              names_prefix = "dereg_", values_fill = list(dereg_YEAR = 0))|>
  mutate(across(starts_with("dereg_"), ~ ifelse(. != 0, 1, 0)))|>
  mutate(earlyT = dereg_1995 + dereg_1996 + dereg_1997)|>
  mutate(lateT = dereg_1999 + dereg_2000)|>
  mutate(untreated = as.numeric(!(STNAME %in% dereged_states)))|>
  mutate(earlyG = as.numeric(STNAME %in% earlyT_states))|>
  mutate(lateG = as.numeric(STNAME %in% lateT_states))

DID_1<- lmer(ROA ~ first_dereg + (1 | YEARf)  + (1 | STNAME) + (1 | CERT), data = lm_data)
summary(DID_1)
# estimated effect of both treatments: -0.001816
```
3
Note that I include STNAME in the regression. STNAME is to account for local state effects. QUARTER used to be in the regression to account for seasonal effects. However, The model failed to converge, so I removed it from this and every subsequent equation.
I admit to not having a clue if this model is overly reasonable on a general level -I am not familiar with the data generation process (DGP) of the data used for this study- and understanding the DGP is fundamental for defensible modelling. Anyway, I do not consider leaving out the variable by which I cluster a defensible choice under any circumstances, even though it is theoretically possible.
The DID coefficient is given as approximately -0.03 with a standard deviation of 0.03. That is to say, assuming the model is approximately correctly specified, we expect the deregulation of the branching of banks to decrease return over assets on average by 0.3%; if the model is approximately correctly specified, we expect the true effect to lie with a probability of about 95% between -0.09 and 0.03. Considering the 25% quantile of ROA is 0.72 and the 75% quantile is 1.38, it appears that the kind of deregulation we have modeled had negligible effects on the affected banks' ROA.

4
The diff-in-diff estimator in (3) has a problem accounting for the different timing of the treatment. If every US state were modelled entirely independent from each other, this would not be a problem. However, the model ties the states together using random (YEAR) effects. The staggered treatment coincides with three different random effects, as there are three points in time when the treatment (according to this crude model) is administered. According to the lecture slides, this can be formulated as a multiple comparison problem violating the parallel trends assumption.
```{r}
# 5
DID_2<- lmer(ROA ~ dereg_1995 + dereg_1996 + dereg_1997 + dereg_1999 + dereg_2000  + (1 | YEARf)  + (1 | STNAME) + (1 | CERT), data = lm_data)

summary(DID_2)
# estimated effects, 1995 to 2000: 0.045989, 0.019123, 0.006058, -0.117779, -0.110085
```
It appears from the exercise that we were supposed to create only a staggered treatment of two points in time, presumably from the two waves observed in (2). In the following, I estimate the model and make the comparisons using only these two waves. I do not consider this a smart modeling choice, because treatments are now spread across several years. The results are to be taken with a pinch of salt at any rate due to massive forking path issues (Gelman, Andrew and Loken, Eric (2013: The garden of forking paths: Why multiple comparisons can be a problem,even when there is no âfishing expeditionâ or âp-hackingâ and the research hypothesis was posited ahead of time), a problem also referred to as 'researcher degrees of freedom' (Simmons, Joseph and Nelson, Leif and Simonsohn, Uri: False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant).

```{r}
lm_data_A<- lm_data|>
  dplyr::filter(lateG != 1)
lm_data_B<- lm_data|>
  dplyr::filter(earlyG != 1)
lm_data_C<- lm_data|>
  dplyr::filter(YEAR <= 1998)|>
  dplyr::filter(untreated != 1)
lm_data_D<- lm_data|>
  dplyr::filter(YEAR >= 1998)|>
  dplyr::filter(untreated != 1)

# Bacon decomposition (A)
DID_A <- lmer(ROA ~ earlyT + (1 | YEARf)  + (1 | STNAME) + (1 | CERT), data = lm_data_A)
summary(DID_A)
# estimated effect of early treatment: 0.01466

# Bacon decomposition (B)
DID_B <- lmer(ROA ~ lateT + (1 | YEARf)  + (1 | STNAME) + (1 | CERT), data = lm_data_B)
summary(DID_B)
# estimated effect of late treatment: -0.11074
  
# Bacon decomposition (C)
DID_C <- lmer(ROA ~ earlyT + (1 | YEARf)  + (1 | STNAME) + (1 | CERT), data = lm_data_C)
summary(DID_C)
# estimated effect of early treatment: 0.008121

# Bacon decomposition (D)
DID_D <- lmer(ROA ~ lateT + (1 | YEARf)  + (1 | STNAME) + (1 | CERT), data = lm_data_D)
summary(DID_D)
# estimated effect of late treatment: -0.10113

```
As discussed above, the Bacon-Goodman decomposition cannot be applied as the panel is not balanced in the sense of Andrew Bacon-Goodman (2021): Difference-in-differences with variation in treatment timing. It is therefore not possible to show that the estimator from (3) can be decomposed according to the estimates in (5). However, looking at the estimators makes the decompositability of a difference in differences estimator plausible. The largest number of banks is in the early treatment group. The late treatment group is nearly negligibly small. The rest of the mass is in the control group. We may conclude that estimator A must have the largest weight of the four. Estimator B must be weighted very little, as it is based on the late treatment and control groups. Estimators C and D are also based on the early treatment group, which gives them some weight, but not as much as A, as they are restricted in the time domain. 
```{r}
# 6


```
7
The parallel trends assumption is the foundation of difference in differences. In the real world, we observe a treatment and a control group. But we really want, though, is to know 'What would have happened if the treatment group had not been treated? How much of a difference does the treatment make on the outcome?' To answer this, one may employ an untreated proxy. The proxy is in some ways similar and in some ways different to the treated unit. Because of those differences, we cannot say 'Had the treated unit not been treated, it would have ended like the control unit!' However, due to the similarities between treatment and control unit, we may be willing to bite the bullet of accepting parallel trends. Parallel trends mean that even though the status of the control/ treatment group might differ, their dynamics are similar. DID assumes identical, linear dynamics. Whether this is a defensible modeling choice must be evaluated case-by-case. 
```{r}
# https://github.com/jonathandroth/pretrends/blob/master/README.md
slope80<- slope_for_power(sigma = NA, targetPower = 0.8, tVec = NA, referencePeriod = NA)
pretrends(betahat = NA, sigma = NA, deltatrue = slope80, tVec = NA)

```

